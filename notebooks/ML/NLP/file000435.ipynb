{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0-rc2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "import re\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_lines = open(r'InsuranceQnA-master\\vocabulary.txt', encoding='utf-8', errors=\"ignore\").read().split('\\n')\n",
    "question_lines = open(r'InsuranceQnA-master\\InsuranceQAquestionanslabelraw.encoded', encoding='utf-8', errors=\"ignore\").read().split('\\n')\n",
    "answer_lines = open(r'InsuranceQnA-master\\InsuranceQAlabel2answerraw.encoded', encoding='utf-8', errors=\"ignore\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = {}\n",
    "for line in vocab_lines:\n",
    "    _line = line.split('\\t')\n",
    "    if len(_line) == 2:\n",
    "        id2line[_line[0]] = _line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs, ansid = [], []\n",
    "for line in question_lines[:-1]:\n",
    "    _line = line.split('\\t')\n",
    "    ansid.append(_line[2].split(' '))\n",
    "    convs.append(_line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medicare-insurance',\n",
       " 'idx_1285 idx_1010 idx_467 idx_47610 idx_18488 idx_65760',\n",
       " '16696']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_lines[0].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs1 = [ ]\n",
    "for line in answer_lines[:-1]:\n",
    "    _line = line.split('\\t')\n",
    "    convs1.append(_line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['idx_1285 idx_1010 idx_467 idx_47610 idx_18488 idx_65760', 'idx_3815 idx_604 idx_605 idx_891 idx_136 idx_5293 idx_65761']\n",
      "[['16696'], ['10277']]\n",
      "['idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 idx_10 idx_11 idx_12 idx_13 idx_14 idx_3 idx_12 idx_15 idx_16 idx_17 idx_8 idx_18 idx_19 idx_20 idx_21 idx_3 idx_12 idx_14 idx_22 idx_20 idx_23 idx_24 idx_25 idx_26 idx_27 idx_28 idx_29 idx_8 idx_30 idx_19 idx_11 idx_4 idx_31 idx_32 idx_22 idx_33 idx_34 idx_35 idx_36 idx_37 idx_30 idx_38 idx_39 idx_11 idx_40 idx_41 idx_42 idx_43 idx_44 idx_22 idx_45 idx_46 idx_11 idx_47 idx_48 idx_49 idx_18 idx_50 idx_20 idx_44 idx_22 idx_51 idx_14 idx_52 idx_53 idx_22 idx_40 idx_21 idx_3 idx_54 idx_46 idx_11 idx_55 idx_56 idx_57 idx_58 idx_59 idx_60 idx_61 idx_62 idx_8 idx_50 idx_11 idx_45 idx_63 idx_64 idx_3 idx_65 idx_66 idx_3 idx_67 idx_68 idx_69 idx_70 idx_14 idx_3 idx_71 idx_72 idx_73 idx_21 idx_74 idx_5 idx_75 idx_76 idx_12 idx_8 idx_77 idx_78 idx_15 idx_79 idx_49 idx_18 idx_19 idx_11 idx_54 idx_44 idx_22 idx_80 idx_53 idx_3 idx_12 idx_21 idx_81 idx_54 idx_14 idx_82 idx_83 idx_41 idx_3 idx_84 idx_22 idx_54 idx_23 idx_85 idx_11 idx_86 idx_87 idx_88 idx_89 idx_3 idx_90 idx_41 idx_3 idx_47 idx_12 idx_20 idx_77 idx_91 idx_22 idx_92 idx_93 idx_94 idx_82 idx_14 idx_95 idx_96 idx_97 idx_3 idx_98 idx_99 idx_100 idx_101 idx_3 idx_102 idx_103 idx_104 idx_105 idx_106 idx_107 idx_108 idx_14 idx_109 idx_110 idx_30 idx_111 idx_3 idx_92 idx_112 idx_3 idx_68 idx_113 idx_114 idx_115 idx_52 idx_116 idx_11 idx_10 idx_113 idx_117 idx_118 idx_119 idx_77 idx_120 idx_121 idx_122 idx_41 idx_11 idx_123', 'idx_124 idx_107 idx_11 idx_125 idx_126 idx_127 idx_128 idx_129 idx_81 idx_8 idx_130 idx_30 idx_131 idx_129 idx_97 idx_22 idx_132 idx_133 idx_134 idx_135 idx_41 idx_136 idx_23 idx_130 idx_82 idx_137 idx_138 idx_3 idx_139 idx_140 idx_90 idx_97 idx_141 idx_142 idx_143 idx_3 idx_144 idx_14 idx_97 idx_145 idx_3 idx_146 idx_49 idx_147 idx_148 idx_30 idx_149 idx_119 idx_97 idx_22 idx_150 idx_76 idx_151 idx_81 idx_22 idx_152 idx_153 idx_154 idx_39 idx_155 idx_156 idx_157 idx_3 idx_54 idx_107 idx_61 idx_11 idx_158 idx_159 idx_97 idx_3 idx_160 idx_161 idx_162 idx_163 idx_107 idx_81 idx_119 idx_164 idx_11 idx_165 idx_166 idx_167 idx_81 idx_168 idx_30 idx_3 idx_157 idx_41 idx_3 idx_169 idx_14 idx_170 idx_171 idx_172 idx_82 idx_3 idx_173 idx_174 idx_124 idx_107 idx_175 idx_176 idx_61 idx_3 idx_59 idx_177 idx_81 idx_107 idx_178 idx_179 idx_180 idx_74 idx_181 idx_23 idx_182 idx_97 idx_183 idx_184 idx_185 idx_87 idx_186 idx_187 idx_188 idx_187 idx_189 idx_190 idx_179 idx_191 idx_30 idx_192 idx_193 idx_3 idx_194 idx_164 idx_195 idx_87 idx_196 idx_89 idx_197 idx_198 idx_199 idx_41 idx_3 idx_200 idx_3 idx_201 idx_202 idx_203 idx_204 idx_205 idx_206 idx_3 idx_201 idx_30 idx_207 idx_66 idx_208 idx_209 idx_14 idx_210 idx_211 idx_212 idx_213 idx_212 idx_214 idx_215 idx_216 idx_178 idx_217 idx_218 idx_107 idx_219 idx_57 idx_220 idx_59 idx_8 idx_23 idx_221 idx_11 idx_222 idx_97 idx_171 idx_223 idx_224 idx_23 idx_225 idx_24 idx_97 idx_226 idx_70 idx_41 idx_3 idx_169 idx_193 idx_179 idx_227 idx_82 idx_22 idx_228 idx_87 idx_97 idx_229 idx_230 idx_231 idx_232 idx_82 idx_233 idx_234 idx_3 idx_235 idx_41 idx_140 idx_3 idx_54 idx_230 idx_231 idx_236 idx_14 idx_171 idx_71 idx_41 idx_237 idx_193 idx_179 idx_238 idx_239 idx_3 idx_240 idx_24 idx_3 idx_59 idx_8 idx_241 idx_130 idx_3 idx_242 idx_30 idx_243 idx_177 idx_244 idx_191 idx_11 idx_36 idx_245 idx_14 idx_171 idx_246 idx_30 idx_247 idx_3 idx_248 idx_245 idx_14 idx_249 idx_3 idx_250 idx_7 idx_8 idx_251 idx_81 idx_252 idx_231 idx_253 idx_254 idx_255 idx_8 idx_130 idx_3 idx_242 idx_256 idx_30 idx_257 idx_11 idx_258 idx_116 idx_3 idx_259 idx_7 idx_8 idx_241 idx_260 idx_24 idx_261 idx_30 idx_22 idx_262 idx_104 idx_263 idx_179 idx_264 idx_8 idx_30 idx_265 idx_3 idx_266 idx_87 idx_130 idx_8 idx_267 idx_268 idx_269 idx_81 idx_270 idx_271 idx_99 idx_18 idx_272 idx_8 idx_179 idx_230 idx_16 idx_273 idx_82 idx_3 idx_274 idx_81 idx_164 idx_275 idx_254 idx_3 idx_276 idx_57 idx_277 idx_179 idx_23 idx_130 idx_231 idx_278 idx_97 idx_11 idx_279 idx_280 idx_14 idx_93 idx_130 idx_230 idx_30 idx_281 idx_161 idx_282 idx_41 idx_22 idx_152 idx_283 idx_284 idx_128 idx_129 idx_97 idx_22 idx_285 idx_14 idx_99 idx_286 idx_218 idx_287 idx_288 idx_97 idx_289 idx_290 idx_8 idx_97 idx_291']\n"
     ]
    }
   ],
   "source": [
    "print(convs[:2])\n",
    "print(ansid[:2])\n",
    "print(convs1[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "questions, answers = [], []\n",
    "for a in range(len(ansid)):\n",
    "      for b in range(len(ansid[a])):\n",
    "#             print(count)\n",
    "#             print(a,b)\n",
    "            questions.append(convs[a])\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(len(ansid)):\n",
    "      for b in range(len(ansid[a])):\n",
    "            answers.append(convs1[int(ansid[a][b])-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample code for above task in my version\n",
    "# ques=[]\n",
    "# for i in questions:\n",
    "#     a=[]\n",
    "#     que_tokens=i.split(' ')\n",
    "#     for token in que_tokens:\n",
    "#         word=id2line[token.strip()]\n",
    "#         a.append(word)\n",
    "#     ques.append(' '.join(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques, ans  =[], []\n",
    "m=0\n",
    "while m<len(questions):\n",
    "       i=0\n",
    "       a=[]\n",
    "       while i < (len(questions[m].split(' '))):\n",
    "            a.append(id2line[questions[m].split(' ')[i]])\n",
    "            i=i+1\n",
    "       ques.append(' '.join(a))\n",
    "       m=m+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "while n<len(answers):  \n",
    "        j=0\n",
    "        b=[]\n",
    "        while j < (len(answers[n].split(' '))):\n",
    "            b.append(id2line[answers[n].split(' ')[j]])\n",
    "            j=j+1\n",
    "        ans.append(' '.join(b))\n",
    "        n=n+1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        \"\"\"Cleaning the text by replacing the abbreviated words with their proper full replacement, and converting all the characters to lower case\"\"\"\n",
    "\n",
    "        text = text.lower()\n",
    "\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"what's\", \"that is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"how's\", \"how is\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"n't\", \" not\", text)\n",
    "        text = re.sub(r\"n'\", \"ng\", text)\n",
    "        text = re.sub(r\"'bout\", \"about\", text)\n",
    "        text = re.sub(r\"'til\", \"until\", text)\n",
    "        text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,']\", \"\", text)\n",
    "\n",
    "        return text\n",
    "\n",
    "# Applying the 'clean_text()' function on the set of Questions and Answers\n",
    "clean_questions = []\n",
    "for question in ques:\n",
    "    clean_questions.append(clean_text(question))\n",
    "\n",
    "clean_answers = []    \n",
    "for answer in ans:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_line_length, max_line_length = 2, 100\n",
    "short_questions_temp, short_answers_temp = [], []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_questions, short_answers = [], []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19108"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19108"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    \"\"\"Including <PAD> token in sentence to make all batches of same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for question in short_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for answer in short_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 18983\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_vocab_to_int = {}\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "\n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        answers_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1\n",
    "\n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for question in questions_int:\n",
    "    for word in question:\n",
    "        if word == questions_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "\n",
    "for answer in answers_int:\n",
    "    for word in answer:\n",
    "        if word == answers_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "\n",
    "unk_ratio = round(unk_count/word_count,4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[219, 13]\n",
      "[219, 13, 58, 2310, 3636, 1384, 3365, 220, 2332, 219, 13, 243, 55, 394, 166, 219, 13, 243, 55, 72, 76, 135, 1140, 18574, 5, 4624, 26, 1651, 573, 175, 21, 2363, 193, 76, 4126, 312, 811, 105, 18575, 7875, 175, 1514, 193, 18576, 135, 21, 2363, 26, 21, 3636, 219, 13, 21, 2363, 219, 171]\n",
      "why can\n",
      "why can a simple question but yet so complex why can someone do this or why can someone do that i have often pondered for hours to come up with the answer and i believe after years of thoughtprovoking consultation with friends and relativesi have the answer to the question why can the answer why not\n",
      "\n",
      "[133, 479, 56]\n",
      "[242, 4123, 3646, 282, 306, 56, 2627, 3924, 201, 918, 5047, 4443, 5, 176, 56, 5048, 193, 21, 2346, 105, 10, 176, 763, 1166, 176, 56, 201, 1916, 763, 193, 1651, 299, 21, 4273, 105, 5049, 5050, 1384, 3685, 84, 6, 29, 202, 10, 3602, 237, 2458, 84, 2379, 299, 21, 2346, 105, 10, 288, 5048, 56, 201, 1168, 20, 21, 2346, 105, 10, 32, 669, 153]\n",
      "who governs annuities\n",
      "if youre asking about all annuities then here are two governing bodies for variable annuities finra and the department of insurance variable products like variable annuities are registered products and come under the oversight of finras jurisdiction but because it is an annuity insurance product as well it falls under the department of insurance non finra annuities are governed by the department of insurance in each state\n",
      "\n",
      "[0, 201, 56]\n",
      "[29, 202, 6, 29, 10, 3602, 58, 36, 10, 179, 4187, 93, 17, 1452, 1989, 409, 29, 202, 4187, 93, 17, 951, 1989, 7, 56, 201, 2332, 4642, 32, 1668, 5, 58, 218, 105, 43, 3707, 395, 166, 32, 5087, 21, 181, 81, 122, 21, 4062, 58, 3272, 320, 1621, 881, 5, 21, 36, 105, 21, 4062, 388, 201, 386, 949, 105, 394, 101, 787, 56, 201, 170, 5088, 17, 643, 771]\n",
      "what are annuities\n",
      "an annuity is an insurance product a life insurance policy protects you from dying too soon an annuity protects you from living too long annuities are complex basically in exchange for a sum of money either immediate or in installments the company will pay the annuitant a specific amount normally monthly for the life of the annuitant there are many modifications of this basic form annuities are taxed differently from other programs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_questions = []\n",
    "short_questions1 = []\n",
    "sorted_answers = []\n",
    "short_answers1= []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for i in enumerate(questions_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_questions.append(questions_int[i[0]])\n",
    "            short_questions1.append(short_questions[i[0]])\n",
    "            sorted_answers.append(answers_int[i[0]])\n",
    "            short_answers1.append(short_answers[i[0]])\n",
    "\n",
    "for i in range(3):\n",
    "    print(sorted_questions[i])\n",
    "    print(sorted_answers[i])\n",
    "    print(short_questions1[i])\n",
    "    print(short_answers1[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "    lr = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    return input_data, targets, lr, keep_prob\n",
    "\n",
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input\n",
    "\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell, cell_bw = enc_cell, sequence_length = sequence_length, inputs = rnn_inputs, dtype=tf.float32)\n",
    "    return enc_state\n",
    "\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "\n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "\n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = tf.keras.seq2seq.prepare_attention(attention_states, attention_option=\"bahdanau\", num_units=dec_cell.output_size)\n",
    "\n",
    "    train_decoder_fn = tf.keras.seq2seq.attention_decoder_fn_train(encoder_state[0], att_keys, att_vals,  att_score_fn, att_construct_fn,  name = \"attn_dec_train\")\n",
    "\n",
    "    train_pred, _, _ = tf.keras.seq2seq.dynamic_rnn_decoder(dec_cell, train_decoder_fn,  dec_embed_input, sequence_length, scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "\n",
    "    return output_fn(train_pred_drop)\n",
    "\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "\n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "\n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option=\"bahdanau\", num_units=dec_cell.output_size)\n",
    "\n",
    "    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0], att_keys, att_vals,  att_score_fn, att_construct_fn,  name = \"attn_dec_train\")\n",
    "\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, train_decoder_fn,  dec_embed_input, sequence_length, scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "\n",
    "    return output_fn(train_pred_drop)\n",
    "\n",
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "\n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "\n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option=\"bahdanau\", num_units=dec_cell.output_size)\n",
    "\n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, encoder_state[0],  att_keys, att_vals,  att_score_fn, att_construct_fn,\n",
    "                        dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, name = \"attn_dec_inf\")\n",
    "\n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, infer_decoder_fn, scope=decoding_scope)\n",
    "\n",
    "    return infer_logits\n",
    "\n",
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, vocab_to_int, keep_prob, batch_size):\n",
    "\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "\n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, vocab_size, None,  scope=decoding_scope, weights_initializer = weights, biases_initializer = biases)\n",
    "\n",
    "        train_logits = decoding_layer_train(encoder_state, dec_cell,  dec_embed_input, sequence_length,  decoding_scope, output_fn, keep_prob, batch_size)\n",
    "\n",
    "        decoding_scope.reuse_variables()\n",
    "        infer_logits = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, vocab_to_int['<GO>'], vocab_to_int['<EOS>'],\n",
    "                    sequence_length - 1, vocab_size,  decoding_scope, output_fn, keep_prob, batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size,\n",
    "                  questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers,\n",
    "                  questions_vocab_to_int):\n",
    "\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, answers_vocab_size+1,  enc_embedding_size, initializer = tf.random_uniform_initializer(0,1))\n",
    "\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "\n",
    "    dec_input = process_encoding_input(target_data, questions_vocab_to_int, batch_size)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([questions_vocab_size+1, dec_embedding_size], 0, 1))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    train_logits, infer_logits = decoding_layer(dec_embed_input, dec_embeddings, enc_state, questions_vocab_size,\n",
    "                            sequence_length, rnn_size, num_layers, questions_vocab_to_int,  keep_prob, batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### need to modify th code\n",
    "https://stackoverflow.com/questions/41333798/attributeerror-module-tensorflow-has-no-attribute-interactivesession\n",
    "https://stackoverflow.com/questions/40782271/attributeerror-module-tensorflow-has-no-attribute-reset-default-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Starting the session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops.reset_default_graph()\n",
    "# # Starting the session\n",
    "# sess = tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, targets, lr, keep_prob = model_inputs()\n",
    "sequence_length = tf.placeholder_with_default(max_line_length, None, name=\"sequence_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.seq2seq' has no attribute 'prepare_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-869bdb2566b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m train_logits, inference_logits = seq2seq_model( tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int),\n\u001b[1;32m----> 4\u001b[1;33m     len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers,  questions_vocab_to_int)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-e9f694f678a0>\u001b[0m in \u001b[0;36mseq2seq_model\u001b[1;34m(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size, questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, questions_vocab_to_int)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     train_logits, infer_logits = decoding_layer(dec_embed_input, dec_embeddings, enc_state, questions_vocab_size,\n\u001b[1;32m---> 14\u001b[1;33m                             sequence_length, rnn_size, num_layers, questions_vocab_to_int,  keep_prob, batch_size)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-ce7d86b2e8fb>\u001b[0m in \u001b[0;36mdecoding_layer\u001b[1;34m(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size, num_layers, vocab_to_int, keep_prob, batch_size)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0moutput_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoding_scope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbiases_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mtrain_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoding_layer_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_cell\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mdec_embed_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mdecoding_scope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mdecoding_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-ce7d86b2e8fb>\u001b[0m in \u001b[0;36mdecoding_layer_train\u001b[1;34m(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob, batch_size)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mattention_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0matt_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_score_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_construct_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_option\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bahdanau\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdec_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mtrain_decoder_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_decoder_fn_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_vals\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0matt_score_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_construct_fn\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"attn_dec_train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.contrib.seq2seq' has no attribute 'prepare_attention'"
     ]
    }
   ],
   "source": [
    "input_shape = tf.shape(input_data)\n",
    "\n",
    "train_logits, inference_logits = seq2seq_model( tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int),\n",
    "    len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers,  questions_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.init_ops.RandomUniform object at 0x000001F4BEAC2E48>\n"
     ]
    }
   ],
   "source": [
    "print(tf.random_uniform_initializer(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "from tkinter import *\n",
    "\n",
    "\n",
    "def send():\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",END)\n",
    "    if msg != '':\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "        ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ))\n",
    "\n",
    "        res = chatbot_response(msg)\n",
    "        ChatLog.insert(END, \"Bot: \" + res + '\\n\\n')\n",
    "\n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "        \n",
    "        \n",
    "base = Tk()\n",
    "base.title(\"Hello\")\n",
    "base.geometry(\"400x500\")\n",
    "base.resizable(width=FALSE, height=FALSE)\n",
    "\n",
    "\n",
    "#Create Chat window\n",
    "ChatLog = Text(base, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\",)\n",
    "\n",
    "ChatLog.config(state=DISABLED)\n",
    "\n",
    "#Bind scrollbar to Chat window\n",
    "scrollbar = Scrollbar(base, command=ChatLog.yview, cursor=\"heart\")\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "#Create Button to send message\n",
    "SendButton = Button(base, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5,\n",
    "                    bd=0, bg=\"#32de97\", activebackground=\"#3c9d9b\",fg='#ffffff',\n",
    "                    command= send )\n",
    "\n",
    "#Create the box to enter message\n",
    "EntryBox = Text(base, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\", background=\"#dddddd\")\n",
    "#EntryBox.bind(\"<Return>\", send)\n",
    "\n",
    "\n",
    "#Place all components on the screen\n",
    "scrollbar.place(x=376,y=6, height=386)\n",
    "ChatLog.place(x=6,y=6, height=386, width=370)\n",
    "EntryBox.place(x=130, y=401, height=40, width=265)\n",
    "SendButton.place(x=6, y=401, height=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-3-eb280a6039dd>\", line 13, in send\n",
      "    res = chatbot_response(msg)\n",
      "NameError: name 'chatbot_response' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-3-eb280a6039dd>\", line 13, in send\n",
      "    res = chatbot_response(msg)\n",
      "NameError: name 'chatbot_response' is not defined\n"
     ]
    }
   ],
   "source": [
    "base.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
