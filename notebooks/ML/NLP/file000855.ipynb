{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createStopWords(filename):\n",
    "    stopWords = []\n",
    "    with open(filename, \"r\") as file_object:\n",
    "        for line in file_object:\n",
    "            stopWords.append(line.rstrip()) \n",
    "    return stopWords\n",
    "\n",
    "def readFilesAndReturnCleanList(stopWords):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~/+-1234567890'''\n",
    "    cleanDocuments = {}\n",
    "    individualDocument = []\n",
    "    for file in os.listdir('data/'):\n",
    "        if file.endswith(\".txt\"):\n",
    "            documentName = os.path.join(file)\n",
    "            with open(\"data/\" + documentName, \"r\") as document:\n",
    "                for line in document:\n",
    "                    cleanLine = line.rstrip()\n",
    "                    listofWordsInALine = cleanLine.split(\" \")\n",
    "                    cleanWord = \"\"\n",
    "                    ps = PorterStemmer()\n",
    "                    for word in listofWordsInALine:\n",
    "                        if word.lower() not in stopWords:\n",
    "                            word = ps.stem(word)\n",
    "                            for letter in word:\n",
    "                                if letter not in punctuations:\n",
    "                                    cleanWord = cleanWord + letter\n",
    "                            if (cleanWord != \"\"):\n",
    "                                individualDocument.append(cleanWord)             \n",
    "                            cleanWord = \"\"\n",
    "                cleanDocuments[documentName] = individualDocument \n",
    "                individualDocument = [] \n",
    "    return cleanDocuments  \n",
    "\n",
    "def createUniqueWordsList(documentsInADictionary):\n",
    "    listOfUniqueWords = []\n",
    "    for document, words in documentsInADictionary.items():\n",
    "        for word in words:\n",
    "            if word not in listOfUniqueWords:\n",
    "                listOfUniqueWords.append(word)\n",
    "    return listOfUniqueWords\n",
    "\n",
    "def termFrequency(documentName, document):\n",
    "    lengthOfDocument = len(document)\n",
    "    wordInformation = []\n",
    "    wordInfoDictionary = {} \n",
    "    documentAndWordInfo = {}\n",
    "    wordCounter = Counter(document)\n",
    "    if lengthOfDocument != 0:\n",
    "        for word, repetation in wordCounter.items():\n",
    "            frequency = repetation / lengthOfDocument\n",
    "            # wordInformation.append(repetation)\n",
    "            wordInformation.append(frequency)\n",
    "            # wordInformation.append(lengthOfDocument)\n",
    "\n",
    "            wordInfoDictionary[word] = wordInformation\n",
    "            wordInformation = [] \n",
    "    documentAndWordInfo[documentName] = wordInfoDictionary\n",
    "    return documentAndWordInfo\n",
    "\n",
    "def findTermFrequency(documentsInADictionary):\n",
    "    tf = []\n",
    "    for documentName, document in documentsInADictionary.items():\n",
    "        documentAndWordInfo = termFrequency(documentName, document)\n",
    "        tf.append(documentAndWordInfo)\n",
    "    return tf\n",
    "\n",
    "def findInverseDocumentFrequency(listOfUniqueWords, documentsInADictionary, tf):\n",
    "    idf = {}\n",
    "    numberOfDocuments = len(documentsInADictionary)\n",
    "    for word in listOfUniqueWords:\n",
    "        for document, words in documentsInADictionary.items():\n",
    "            if word in words:\n",
    "                if word in idf:\n",
    "                    idf[word] =  idf[word] + 1\n",
    "                else:\n",
    "                    idf[word] = 1\n",
    "    for word, repetation in idf.items():\n",
    "        idf[word] = math.log10(numberOfDocuments / repetation)\n",
    "\n",
    "    return idf\n",
    "\n",
    "def tfIdf(tf, idf):\n",
    "    tfIdfReturnDictionary = {}\n",
    "    listForWordAndValue = []\n",
    "    mainDocumentList = []\n",
    "\n",
    "    for documentAndWords in tf:\n",
    "        documentName = \"\"\n",
    "        for document, wordsInfo in documentAndWords.items():\n",
    "            documentName = document\n",
    "            if len(wordsInfo) != 0:\n",
    "                for word, wordTf in wordsInfo.items():\n",
    "                    tfIdfValue = wordTf[0] * idf[word]\n",
    "\n",
    "                    listForWordAndValue.append(word)\n",
    "                    listForWordAndValue.append(tfIdfValue)\n",
    "                    mainDocumentList.append(listForWordAndValue)\n",
    "                    listForWordAndValue = []\n",
    "        mainDocumentList = sorted(mainDocumentList, key=itemgetter(1), reverse=True)\n",
    "        tfIdfReturnDictionary[documentName] = mainDocumentList\n",
    "        documentName = \"\"\n",
    "        mainDocumentList = []\n",
    "    return tfIdfReturnDictionary\n",
    "\n",
    "def createInputFileForAprioriAlgorithm(tf_idf, topN):\n",
    "    count = 0\n",
    "    writeToFile = open(\"aprioriInput.txt\", \"w\") \n",
    "    for documentName, values in tf_idf.items():\n",
    "        length = len(values[:topN])\n",
    "        if len(values) != 0:\n",
    "            for i in values[:topN]:\n",
    "                count = count + 1\n",
    "                if count == length:\n",
    "                    writeToFile.write(i[0])\n",
    "                else:\n",
    "                    writeToFile.write(i[0] + \", \")\n",
    "            count = 0\n",
    "            writeToFile.write(\"\\n\")\n",
    "\n",
    "def generateC1(dataSet):\n",
    "    productDict = {}\n",
    "    returneSet = []\n",
    "    for data in dataSet:\n",
    "        for product in data:\n",
    "            if product not in productDict:\n",
    "               productDict[product] = 1\n",
    "            else:\n",
    "                 productDict[product] = productDict[product] + 1\n",
    "    for key in productDict:\n",
    "        tempArray = []\n",
    "        tempArray.append(key)\n",
    "        returneSet.append(tempArray)\n",
    "        returneSet.append(productDict[key])\n",
    "        tempArray = []\n",
    "    return returneSet\n",
    "\n",
    "def generateFrequentItemSet(CandidateList, noOfTransactions, minimumSupport, dataSet, fatherFrequentArray):\n",
    "    frequentItemsArray = []\n",
    "    for i in range(len(CandidateList)):\n",
    "        if i%2 != 0:\n",
    "            support = (CandidateList[i] * 1.0 / noOfTransactions) * 100\n",
    "            if support >= minimumSupport:\n",
    "                frequentItemsArray.append(CandidateList[i-1])\n",
    "                frequentItemsArray.append(CandidateList[i])\n",
    "            else:\n",
    "                eleminatedItemsArray.append(CandidateList[i-1])\n",
    "\n",
    "    for k in frequentItemsArray:\n",
    "        fatherFrequentArray.append(k)\n",
    "\n",
    "    if len(frequentItemsArray) == 2 or len(frequentItemsArray) == 0:\n",
    "        #print(\"This will be returned\")\n",
    "        returnArray = fatherFrequentArray\n",
    "        return returnArray\n",
    "\n",
    "    else:\n",
    "        generateCandidateSets(dataSet, eleminatedItemsArray, frequentItemsArray, noOfTransactions, minimumSupport)\n",
    "        \n",
    "def generateCandidateSets(dataSet, eleminatedItemsArray, frequentItemsArray, noOfTransactions, minimumSupport):\n",
    "    onlyElements = []\n",
    "    arrayAfterCombinations = []\n",
    "    candidateSetArray = []\n",
    "    for i in range(len(frequentItemsArray)):\n",
    "        if i%2 == 0:\n",
    "            onlyElements.append(frequentItemsArray[i])\n",
    "    for item in onlyElements:\n",
    "        tempCombinationArray = []\n",
    "        k = onlyElements.index(item)\n",
    "        for i in range(k + 1, len(onlyElements)):\n",
    "            for j in item:\n",
    "                if j not in tempCombinationArray:\n",
    "                    tempCombinationArray.append(j)\n",
    "            for m in onlyElements[i]:\n",
    "                if m not in tempCombinationArray:\n",
    "                    tempCombinationArray.append(m)\n",
    "            arrayAfterCombinations.append(tempCombinationArray)\n",
    "            tempCombinationArray = []\n",
    "    sortedCombinationArray = []\n",
    "    uniqueCombinationArray = []\n",
    "    for i in arrayAfterCombinations:\n",
    "        sortedCombinationArray.append(sorted(i))\n",
    "    for i in sortedCombinationArray:\n",
    "        if i not in uniqueCombinationArray:\n",
    "            uniqueCombinationArray.append(i)\n",
    "    arrayAfterCombinations = uniqueCombinationArray\n",
    "    for item in arrayAfterCombinations:\n",
    "        count = 0\n",
    "        for transaction in dataSet:\n",
    "            if set(item).issubset(set(transaction)):\n",
    "                count = count + 1\n",
    "        if count != 0:\n",
    "            candidateSetArray.append(item)\n",
    "            candidateSetArray.append(count)\n",
    "    generateFrequentItemSet(candidateSetArray, noOfTransactions, minimumSupport, dataSet, fatherFrequentArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = createStopWords(\"listOfStopWords.txt\")\n",
    "documentsInADictionary = readFilesAndReturnCleanList(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfUniqueWords = createUniqueWordsList(documentsInADictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = findTermFrequency(documentsInADictionary)\n",
    "idf = findInverseDocumentFrequency(listOfUniqueWords, documentsInADictionary, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tfIdf(tf, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter value of N: 1500\n"
     ]
    }
   ],
   "source": [
    "topN = input(\"Enter value of N: \")\n",
    "topN = int(topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "createInputFileForAprioriAlgorithm(tf_idf, topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter minimum Support: 10\n",
      "Enter minimum Confidence: 10\n"
     ]
    }
   ],
   "source": [
    "minimumSupport = input('Enter minimum Support: ')\n",
    "minimumConfidence = input('Enter minimum Confidence: ')\n",
    "\n",
    "minimumSupport = int(minimumSupport)\n",
    "minimumConfidence = int(minimumConfidence)\n",
    "\n",
    "# minimumSupport = 10\n",
    "# minimumConfidence = 10\n",
    "nonFrequentSets = []\n",
    "allFrequentItemSets = []\n",
    "tempFrequentItemSets = []\n",
    "dataSet = []\n",
    "eleminatedItemsArray = []\n",
    "noOfTransactions = 1\n",
    "fatherFrequentArray = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"aprioriInput.txt\") as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.rstrip()\n",
    "    dataSet.append(line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstCandidateSet = generateC1(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequentItemSet = generateFrequentItemSet(firstCandidateSet, noOfTransactions, minimumSupport, dataSet, fatherFrequentArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequentItemSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "df_1=pd.read_csv('5_threat_groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words(\"english\")+list(punctuation)\n",
    "def tokenization(text):\n",
    "    pre_processed_text=[]\n",
    "    words = [w for sent in nltk.sent_tokenize(str(text)) for w in nltk.word_tokenize(sent)]    \n",
    "    tokens=[w for w in words if w.lower() not in stop_words] \n",
    "    tokens=[w.lower() for w in tokens if ((w.isalnum()) & (len(w)>1))]\n",
    "    tokens=[lemmatizer.lemmatize(w, pos =\"v\") for w in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "col=df_1.columns\n",
    "for i in col:\n",
    "    text=df_1[i][0]\n",
    "    tokens= tokenization(text)\n",
    "    docs.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>documentation</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>describe</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>available</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b42</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jury</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jurisdiction</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b2f6d9a62c63f61a6b33dc6520bfcccd</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ﬁﬂ</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10638 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  idf_weights\n",
       "documentation                        1.000000\n",
       "time                                 1.000000\n",
       "describe                             1.000000\n",
       "available                            1.000000\n",
       "15                                   1.000000\n",
       "...                                       ...\n",
       "b42                                  2.098612\n",
       "jury                                 2.098612\n",
       "jurisdiction                         2.098612\n",
       "b2f6d9a62c63f61a6b33dc6520bfcccd     2.098612\n",
       "ﬁﬂ                                   2.098612\n",
       "\n",
       "[10638 rows x 1 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector=cv.transform(docs)\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apt1</th>\n",
       "      <td>0.638520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>0.212312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shanghai</th>\n",
       "      <td>0.168401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>0.156247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61398</th>\n",
       "      <td>0.156122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john356gh</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joeuchill</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audiwheel</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interactions</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10638 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tfidf\n",
       "apt1          0.638520\n",
       "use           0.212312\n",
       "shanghai      0.168401\n",
       "unit          0.156247\n",
       "61398         0.156122\n",
       "...                ...\n",
       "john356gh     0.000000\n",
       "john          0.000000\n",
       "joeuchill     0.000000\n",
       "audiwheel     0.000000\n",
       "interactions  0.000000\n",
       "\n",
       "[10638 rows x 1 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    " \n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    " \n",
    "#print the scores\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
