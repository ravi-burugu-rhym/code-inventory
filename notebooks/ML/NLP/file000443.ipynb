{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import re\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_lines = open(r'InsuranceQnA-master\\vocabulary.txt', encoding='utf-8', errors=\"ignore\").read().split('\\n')\n",
    "question_lines = open(r'InsuranceQnA-master\\InsuranceQAquestionanslabelraw.encoded', encoding='utf-8', errors=\"ignore\").read().split('\\n')\n",
    "answer_lines = open(r'InsuranceQnA-master\\InsuranceQAlabel2answerraw.encoded', encoding='utf-8', errors=\"ignore\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = {}\n",
    "for line in vocab_lines:\n",
    "    _line = line.split('\\t')\n",
    "    if len(_line) == 2:\n",
    "        id2line[_line[0]] = _line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs, ansid = [], []\n",
    "for line in question_lines[:-1]:\n",
    "    _line = line.split('\\t')\n",
    "    ansid.append(_line[2].split(' '))\n",
    "    convs.append(_line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medicare-insurance',\n",
       " 'idx_1285 idx_1010 idx_467 idx_47610 idx_18488 idx_65760',\n",
       " '16696']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_lines[0].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs1 = [ ]\n",
    "for line in answer_lines[:-1]:\n",
    "    _line = line.split('\\t')\n",
    "    convs1.append(_line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['idx_1285 idx_1010 idx_467 idx_47610 idx_18488 idx_65760', 'idx_3815 idx_604 idx_605 idx_891 idx_136 idx_5293 idx_65761']\n",
      "[['16696'], ['10277']]\n",
      "['idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 idx_10 idx_11 idx_12 idx_13 idx_14 idx_3 idx_12 idx_15 idx_16 idx_17 idx_8 idx_18 idx_19 idx_20 idx_21 idx_3 idx_12 idx_14 idx_22 idx_20 idx_23 idx_24 idx_25 idx_26 idx_27 idx_28 idx_29 idx_8 idx_30 idx_19 idx_11 idx_4 idx_31 idx_32 idx_22 idx_33 idx_34 idx_35 idx_36 idx_37 idx_30 idx_38 idx_39 idx_11 idx_40 idx_41 idx_42 idx_43 idx_44 idx_22 idx_45 idx_46 idx_11 idx_47 idx_48 idx_49 idx_18 idx_50 idx_20 idx_44 idx_22 idx_51 idx_14 idx_52 idx_53 idx_22 idx_40 idx_21 idx_3 idx_54 idx_46 idx_11 idx_55 idx_56 idx_57 idx_58 idx_59 idx_60 idx_61 idx_62 idx_8 idx_50 idx_11 idx_45 idx_63 idx_64 idx_3 idx_65 idx_66 idx_3 idx_67 idx_68 idx_69 idx_70 idx_14 idx_3 idx_71 idx_72 idx_73 idx_21 idx_74 idx_5 idx_75 idx_76 idx_12 idx_8 idx_77 idx_78 idx_15 idx_79 idx_49 idx_18 idx_19 idx_11 idx_54 idx_44 idx_22 idx_80 idx_53 idx_3 idx_12 idx_21 idx_81 idx_54 idx_14 idx_82 idx_83 idx_41 idx_3 idx_84 idx_22 idx_54 idx_23 idx_85 idx_11 idx_86 idx_87 idx_88 idx_89 idx_3 idx_90 idx_41 idx_3 idx_47 idx_12 idx_20 idx_77 idx_91 idx_22 idx_92 idx_93 idx_94 idx_82 idx_14 idx_95 idx_96 idx_97 idx_3 idx_98 idx_99 idx_100 idx_101 idx_3 idx_102 idx_103 idx_104 idx_105 idx_106 idx_107 idx_108 idx_14 idx_109 idx_110 idx_30 idx_111 idx_3 idx_92 idx_112 idx_3 idx_68 idx_113 idx_114 idx_115 idx_52 idx_116 idx_11 idx_10 idx_113 idx_117 idx_118 idx_119 idx_77 idx_120 idx_121 idx_122 idx_41 idx_11 idx_123', 'idx_124 idx_107 idx_11 idx_125 idx_126 idx_127 idx_128 idx_129 idx_81 idx_8 idx_130 idx_30 idx_131 idx_129 idx_97 idx_22 idx_132 idx_133 idx_134 idx_135 idx_41 idx_136 idx_23 idx_130 idx_82 idx_137 idx_138 idx_3 idx_139 idx_140 idx_90 idx_97 idx_141 idx_142 idx_143 idx_3 idx_144 idx_14 idx_97 idx_145 idx_3 idx_146 idx_49 idx_147 idx_148 idx_30 idx_149 idx_119 idx_97 idx_22 idx_150 idx_76 idx_151 idx_81 idx_22 idx_152 idx_153 idx_154 idx_39 idx_155 idx_156 idx_157 idx_3 idx_54 idx_107 idx_61 idx_11 idx_158 idx_159 idx_97 idx_3 idx_160 idx_161 idx_162 idx_163 idx_107 idx_81 idx_119 idx_164 idx_11 idx_165 idx_166 idx_167 idx_81 idx_168 idx_30 idx_3 idx_157 idx_41 idx_3 idx_169 idx_14 idx_170 idx_171 idx_172 idx_82 idx_3 idx_173 idx_174 idx_124 idx_107 idx_175 idx_176 idx_61 idx_3 idx_59 idx_177 idx_81 idx_107 idx_178 idx_179 idx_180 idx_74 idx_181 idx_23 idx_182 idx_97 idx_183 idx_184 idx_185 idx_87 idx_186 idx_187 idx_188 idx_187 idx_189 idx_190 idx_179 idx_191 idx_30 idx_192 idx_193 idx_3 idx_194 idx_164 idx_195 idx_87 idx_196 idx_89 idx_197 idx_198 idx_199 idx_41 idx_3 idx_200 idx_3 idx_201 idx_202 idx_203 idx_204 idx_205 idx_206 idx_3 idx_201 idx_30 idx_207 idx_66 idx_208 idx_209 idx_14 idx_210 idx_211 idx_212 idx_213 idx_212 idx_214 idx_215 idx_216 idx_178 idx_217 idx_218 idx_107 idx_219 idx_57 idx_220 idx_59 idx_8 idx_23 idx_221 idx_11 idx_222 idx_97 idx_171 idx_223 idx_224 idx_23 idx_225 idx_24 idx_97 idx_226 idx_70 idx_41 idx_3 idx_169 idx_193 idx_179 idx_227 idx_82 idx_22 idx_228 idx_87 idx_97 idx_229 idx_230 idx_231 idx_232 idx_82 idx_233 idx_234 idx_3 idx_235 idx_41 idx_140 idx_3 idx_54 idx_230 idx_231 idx_236 idx_14 idx_171 idx_71 idx_41 idx_237 idx_193 idx_179 idx_238 idx_239 idx_3 idx_240 idx_24 idx_3 idx_59 idx_8 idx_241 idx_130 idx_3 idx_242 idx_30 idx_243 idx_177 idx_244 idx_191 idx_11 idx_36 idx_245 idx_14 idx_171 idx_246 idx_30 idx_247 idx_3 idx_248 idx_245 idx_14 idx_249 idx_3 idx_250 idx_7 idx_8 idx_251 idx_81 idx_252 idx_231 idx_253 idx_254 idx_255 idx_8 idx_130 idx_3 idx_242 idx_256 idx_30 idx_257 idx_11 idx_258 idx_116 idx_3 idx_259 idx_7 idx_8 idx_241 idx_260 idx_24 idx_261 idx_30 idx_22 idx_262 idx_104 idx_263 idx_179 idx_264 idx_8 idx_30 idx_265 idx_3 idx_266 idx_87 idx_130 idx_8 idx_267 idx_268 idx_269 idx_81 idx_270 idx_271 idx_99 idx_18 idx_272 idx_8 idx_179 idx_230 idx_16 idx_273 idx_82 idx_3 idx_274 idx_81 idx_164 idx_275 idx_254 idx_3 idx_276 idx_57 idx_277 idx_179 idx_23 idx_130 idx_231 idx_278 idx_97 idx_11 idx_279 idx_280 idx_14 idx_93 idx_130 idx_230 idx_30 idx_281 idx_161 idx_282 idx_41 idx_22 idx_152 idx_283 idx_284 idx_128 idx_129 idx_97 idx_22 idx_285 idx_14 idx_99 idx_286 idx_218 idx_287 idx_288 idx_97 idx_289 idx_290 idx_8 idx_97 idx_291']\n"
     ]
    }
   ],
   "source": [
    "print(convs[:2])\n",
    "print(ansid[:2])\n",
    "print(convs1[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "questions, answers = [], []\n",
    "for a in range(len(ansid)):\n",
    "      for b in range(len(ansid[a])):\n",
    "            questions.append(convs[a])\n",
    "            count+=1\n",
    "for a in range(len(ansid)):\n",
    "      for b in range(len(ansid[a])):\n",
    "            answers.append(convs1[int(ansid[a][b])-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample code for above task in my version\n",
    "# ques=[]\n",
    "# for i in questions:\n",
    "#     a=[]\n",
    "#     que_tokens=i.split(' ')\n",
    "#     for token in que_tokens:\n",
    "#         word=id2line[token.strip()]\n",
    "#         a.append(word)\n",
    "#     ques.append(' '.join(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques, ans  =[], []\n",
    "m=0\n",
    "while m<len(questions):\n",
    "       i=0\n",
    "       a=[]\n",
    "       while i < (len(questions[m].split(' '))):\n",
    "            a.append(id2line[questions[m].split(' ')[i]])\n",
    "            i=i+1\n",
    "       ques.append(' '.join(a))\n",
    "       m=m+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "while n<len(answers):  \n",
    "        j=0\n",
    "        b=[]\n",
    "        while j < (len(answers[n].split(' '))):\n",
    "            b.append(id2line[answers[n].split(' ')[j]])\n",
    "            j=j+1\n",
    "        ans.append(' '.join(b))\n",
    "        n=n+1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        \"\"\"Cleaning the text by replacing the abbreviated words with their proper full replacement, and converting all the characters to lower case\"\"\"\n",
    "\n",
    "        text = text.lower()\n",
    "\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"what's\", \"that is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"how's\", \"how is\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"n't\", \" not\", text)\n",
    "        text = re.sub(r\"n'\", \"ng\", text)\n",
    "        text = re.sub(r\"'bout\", \"about\", text)\n",
    "        text = re.sub(r\"'til\", \"until\", text)\n",
    "        text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,']\", \"\", text)\n",
    "\n",
    "        return text\n",
    "\n",
    "# Applying the 'clean_text()' function on the set of Questions and Answers\n",
    "clean_questions = []\n",
    "for question in ques:\n",
    "    clean_questions.append(clean_text(question))\n",
    "\n",
    "clean_answers = []    \n",
    "for answer in ans:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_line_length, max_line_length = 2, 100\n",
    "short_questions_temp, short_answers_temp = [], []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_questions, short_answers = [], []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    \"\"\"Including <PAD> token in sentence to make all batches of same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for question in short_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for answer in short_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_vocab_to_int = {}\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "\n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        answers_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1\n",
    "\n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for question in questions_int:\n",
    "    for word in question:\n",
    "        if word == questions_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "\n",
    "for answer in answers_int:\n",
    "    for word in answer:\n",
    "        if word == answers_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "\n",
    "unk_ratio = round(unk_count/word_count,4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4021, 17664]\n",
      "[4021, 17664, 5205, 2432, 17027, 14142, 6270, 4276, 15811, 4021, 17664, 3681, 1378, 12300, 1646, 4021, 17664, 3681, 1378, 18628, 11454, 18948, 13460, 487, 7648, 10699, 7742, 18074, 18171, 9669, 12817, 6907, 14940, 11454, 18338, 11391, 480, 14065, 8954, 10420, 9669, 5162, 14940, 6060, 18948, 12817, 6907, 7742, 12817, 17027, 4021, 17664, 12817, 6907, 4021, 6202]\n",
      "why can\n",
      "why can a simple question but yet so complex why can someone do this or why can someone do that i have often pondered for hours to come up with the answer and i believe after years of thoughtprovoking consultation with friends and relativesi have the answer to the question why can the answer why not\n",
      "\n",
      "[14553, 4165, 5833]\n",
      "[17329, 16346, 7874, 18478, 17143, 5833, 68, 18070, 9583, 3071, 14090, 964, 7648, 15716, 5833, 10132, 14940, 12817, 12935, 14065, 5515, 15716, 5629, 12284, 15716, 5833, 9583, 5057, 5629, 14940, 18074, 7932, 12817, 18801, 14065, 1327, 4294, 14142, 4543, 13497, 11308, 223, 10561, 5515, 12981, 17451, 14365, 13497, 18544, 7932, 12817, 12935, 14065, 5515, 13863, 10132, 5833, 9583, 17301, 7332, 12817, 12935, 14065, 5515, 10727, 10591, 40]\n",
      "who governs annuities\n",
      "if youre asking about all annuities then here are two governing bodies for variable annuities finra and the department of insurance variable products like variable annuities are registered products and come under the oversight of finras jurisdiction but because it is an annuity insurance product as well it falls under the department of insurance non finra annuities are governed by the department of insurance in each state\n",
      "\n",
      "[16138, 9583, 5833]\n",
      "[223, 10561, 11308, 223, 5515, 12981, 5205, 10176, 5515, 11263, 11183, 11034, 10886, 16243, 8163, 12494, 223, 10561, 11183, 11034, 10886, 7907, 8163, 4712, 5833, 9583, 15811, 1320, 10727, 6706, 7648, 5205, 15165, 14065, 11647, 2904, 6537, 1646, 10727, 6911, 12817, 7412, 12398, 15863, 12817, 14074, 5205, 5640, 8083, 8272, 11289, 7648, 12817, 10176, 14065, 12817, 14074, 5095, 9583, 2857, 4038, 14065, 12300, 17666, 8337, 5833, 9583, 13606, 16876, 10886, 11374, 2621]\n",
      "what are annuities\n",
      "an annuity is an insurance product a life insurance policy protects you from dying too soon an annuity protects you from living too long annuities are complex basically in exchange for a sum of money either immediate or in installments the company will pay the annuitant a specific amount normally monthly for the life of the annuitant there are many modifications of this basic form annuities are taxed differently from other programs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_questions = []\n",
    "short_questions1 = []\n",
    "sorted_answers = []\n",
    "short_answers1= []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for i in enumerate(questions_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_questions.append(questions_int[i[0]])\n",
    "            short_questions1.append(short_questions[i[0]])\n",
    "            sorted_answers.append(answers_int[i[0]])\n",
    "            short_answers1.append(short_answers[i[0]])\n",
    "\n",
    "for i in range(3):\n",
    "    print(sorted_questions[i])\n",
    "    print(sorted_answers[i])\n",
    "    print(short_questions1[i])\n",
    "    print(short_answers1[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "    lr = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    return input_data, targets, lr, keep_prob\n",
    "\n",
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input\n",
    "\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell, cell_bw = enc_cell, sequence_length = sequence_length, inputs = rnn_inputs, dtype=tf.float32)\n",
    "    return enc_state\n",
    "\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "\n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "\n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = tf.keras.seq2seq.prepare_attention(attention_states, attention_option=\"bahdanau\", num_units=dec_cell.output_size)\n",
    "\n",
    "    train_decoder_fn = tf.keras.seq2seq.attention_decoder_fn_train(encoder_state[0], att_keys, att_vals,  att_score_fn, att_construct_fn,  name = \"attn_dec_train\")\n",
    "\n",
    "    train_pred, _, _ = tf.keras.seq2seq.dynamic_rnn_decoder(dec_cell, train_decoder_fn,  dec_embed_input, sequence_length, scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "\n",
    "    return output_fn(train_pred_drop)\n",
    "\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "\n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "\n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option=\"bahdanau\", num_units=dec_cell.output_size)\n",
    "\n",
    "    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0], att_keys, att_vals,  att_score_fn, att_construct_fn,  name = \"attn_dec_train\")\n",
    "\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, train_decoder_fn,  dec_embed_input, sequence_length, scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "\n",
    "    return output_fn(train_pred_drop)\n",
    "\n",
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "\n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "\n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option=\"bahdanau\", num_units=dec_cell.output_size)\n",
    "\n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, encoder_state[0],  att_keys, att_vals,  att_score_fn, att_construct_fn,\n",
    "                        dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, name = \"attn_dec_inf\")\n",
    "\n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, infer_decoder_fn, scope=decoding_scope)\n",
    "\n",
    "    return infer_logits\n",
    "\n",
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, vocab_to_int, keep_prob, batch_size):\n",
    "\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "\n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, vocab_size, None,  scope=decoding_scope, weights_initializer = weights, biases_initializer = biases)\n",
    "\n",
    "        train_logits = decoding_layer_train(encoder_state, dec_cell,  dec_embed_input, sequence_length,  decoding_scope, output_fn, keep_prob, batch_size)\n",
    "\n",
    "        decoding_scope.reuse_variables()\n",
    "        infer_logits = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, vocab_to_int['<GO>'], vocab_to_int['<EOS>'],\n",
    "                    sequence_length - 1, vocab_size,  decoding_scope, output_fn, keep_prob, batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size,\n",
    "                  questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers,\n",
    "                  questions_vocab_to_int):\n",
    "\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, answers_vocab_size+1,  enc_embedding_size, initializer = tf.random_uniform_initializer(0,1))\n",
    "\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "\n",
    "    dec_input = process_encoding_input(target_data, questions_vocab_to_int, batch_size)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([questions_vocab_size+1, dec_embedding_size], 0, 1))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    train_logits, infer_logits = decoding_layer(dec_embed_input, dec_embeddings, enc_state, questions_vocab_size,\n",
    "                            sequence_length, rnn_size, num_layers, questions_vocab_to_int,  keep_prob, batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Starting the session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, targets, lr, keep_prob = model_inputs()\n",
    "sequence_length = tf.placeholder_with_default(max_line_length, None, name=\"sequence_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tf.shape(input_data)\n",
    "\n",
    "train_logits, inference_logits = seq2seq_model( tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int),\n",
    "    len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers,  questions_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.identity(inference_logits, 'logits')\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Calculating Loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss( train_logits, targets, tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "    # Using Adam Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    # Performing Gradient Clipping to handle the vanishing gradient problem\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(questions, answers, batch_size):\n",
    "\n",
    "    for batch_i in range(0, len(questions)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        yield pad_questions_batch, pad_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16242\n",
      "2866\n"
     ]
    }
   ],
   "source": [
    "train_valid_split = int(len(sorted_questions)*0.15)\n",
    "\n",
    "train_questions = sorted_questions[train_valid_split:]\n",
    "train_answers = sorted_answers[train_valid_split:]\n",
    "\n",
    "valid_questions = sorted_questions[:train_valid_split]\n",
    "valid_answers = sorted_answers[:train_valid_split]\n",
    "\n",
    "print(len(train_questions))\n",
    "print(len(valid_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 20        # Check training loss after every 20 batches\n",
    "\n",
    "stop_early = 0\n",
    "\n",
    "stop = 5                 # If the validation loss decreases after 5 consecutive checks, stop training\n",
    "\n",
    "validation_check = ((len(train_questions))//batch_size//2)-1        # Counter for checking validation loss\n",
    "\n",
    "total_train_loss = 0     # Record the training loss for each display step\n",
    "\n",
    "summary_valid_loss = []     # Record the validation loss for saving improvements in the model\n",
    "\n",
    "checkpoint= \"./best_model.ckpt\"   # creating the checkpoint file in the current directory\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 Batch    0/253 - Loss:  0.498, Seconds: 364.91\n",
      "Epoch   1/50 Batch   20/253 - Loss:  8.311, Seconds: 361.21\n",
      "Epoch   1/50 Batch   40/253 - Loss:  4.567, Seconds: 380.52\n",
      "Epoch   1/50 Batch   60/253 - Loss:  4.404, Seconds: 478.50\n",
      "Epoch   1/50 Batch   80/253 - Loss:  4.480, Seconds: 456.56\n",
      "Epoch   1/50 Batch  100/253 - Loss:  4.371, Seconds: 383.58\n",
      "Epoch   1/50 Batch  120/253 - Loss:  4.324, Seconds: 307.75\n",
      "Valid Loss:  4.130, Seconds: 228.40\n",
      "New Record!\n",
      "Epoch   1/50 Batch  140/253 - Loss:  4.292, Seconds: 385.96\n",
      "Epoch   1/50 Batch  160/253 - Loss:  4.271, Seconds: 384.78\n",
      "Epoch   1/50 Batch  180/253 - Loss:  4.201, Seconds: 379.80\n",
      "Epoch   1/50 Batch  200/253 - Loss:  4.118, Seconds: 386.72\n",
      "Epoch   1/50 Batch  220/253 - Loss:  4.046, Seconds: 389.18\n",
      "Epoch   1/50 Batch  240/253 - Loss:  4.015, Seconds: 386.88\n",
      "Valid Loss:  3.935, Seconds: 273.58\n",
      "New Record!\n",
      "Epoch   2/50 Batch    0/253 - Loss:  2.580, Seconds: 348.58\n",
      "Epoch   2/50 Batch   20/253 - Loss:  3.965, Seconds: 377.47\n",
      "Epoch   2/50 Batch   40/253 - Loss:  3.877, Seconds: 375.47\n",
      "Epoch   2/50 Batch   60/253 - Loss:  3.805, Seconds: 363.13\n",
      "Epoch   2/50 Batch   80/253 - Loss:  3.899, Seconds: 377.13\n",
      "Epoch   2/50 Batch  100/253 - Loss:  3.834, Seconds: 362.00\n",
      "Epoch   2/50 Batch  120/253 - Loss:  3.815, Seconds: 360.88\n",
      "Valid Loss:  3.665, Seconds: 275.26\n",
      "New Record!\n",
      "Epoch   2/50 Batch  140/253 - Loss:  3.791, Seconds: 380.72\n",
      "Epoch   2/50 Batch  160/253 - Loss:  3.780, Seconds: 379.18\n",
      "Epoch   2/50 Batch  180/253 - Loss:  3.746, Seconds: 369.17\n",
      "Epoch   2/50 Batch  200/253 - Loss:  3.703, Seconds: 377.27\n",
      "Epoch   2/50 Batch  220/253 - Loss:  3.685, Seconds: 380.40\n",
      "Epoch   2/50 Batch  240/253 - Loss:  3.676, Seconds: 380.10\n",
      "Valid Loss:  3.668, Seconds: 6921.45\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch    0/253 - Loss:  2.389, Seconds: 350.86\n",
      "Epoch   3/50 Batch   20/253 - Loss:  3.686, Seconds: 371.41\n",
      "Epoch   3/50 Batch   40/253 - Loss:  3.618, Seconds: 373.35\n",
      "Epoch   3/50 Batch   60/253 - Loss:  3.571, Seconds: 372.93\n",
      "Epoch   3/50 Batch   80/253 - Loss:  3.658, Seconds: 376.11\n",
      "Epoch   3/50 Batch  100/253 - Loss:  3.603, Seconds: 359.00\n",
      "Epoch   3/50 Batch  120/253 - Loss:  3.587, Seconds: 353.14\n",
      "Valid Loss:  3.474, Seconds: 274.65\n",
      "New Record!\n",
      "Epoch   3/50 Batch  140/253 - Loss:  3.569, Seconds: 370.97\n",
      "Epoch   3/50 Batch  160/253 - Loss:  3.555, Seconds: 372.51\n",
      "Epoch   3/50 Batch  180/253 - Loss:  3.530, Seconds: 367.11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4d5321bd7ffd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             {input_data: questions_batch, targets: answers_batch,  lr: learning_rate,\n\u001b[1;32m----> 8\u001b[1;33m              sequence_length: answers_batch.shape[1], keep_prob: keep_probability})\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\srikanth\\.conda\\envs\\qa_chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\srikanth\\.conda\\envs\\qa_chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\srikanth\\.conda\\envs\\qa_chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\srikanth\\.conda\\envs\\qa_chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\srikanth\\.conda\\envs\\qa_chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_i in range(1, epochs+1):\n",
    "    for batch_i, (questions_batch, answers_batch) in enumerate(\n",
    "            batch_data(train_questions, train_answers, batch_size)):\n",
    "        start_time = time.time()\n",
    "        _, loss = sess.run(\n",
    "            [train_op, cost],\n",
    "            {input_data: questions_batch, targets: answers_batch,  lr: learning_rate,\n",
    "             sequence_length: answers_batch.shape[1], keep_prob: keep_probability})\n",
    "\n",
    "        total_train_loss += loss\n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time\n",
    "\n",
    "        if batch_i % display_step == 0:\n",
    "            print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                  .format(epoch_i, epochs, batch_i,\n",
    "                          len(train_questions) // batch_size, total_train_loss / display_step,\n",
    "                          batch_time*display_step))\n",
    "            total_train_loss = 0\n",
    "\n",
    "        if batch_i % validation_check == 0 and batch_i > 0:\n",
    "            total_valid_loss = 0\n",
    "            start_time = time.time()\n",
    "            for batch_ii, (questions_batch, answers_batch) in enumerate(batch_data(valid_questions, valid_answers, batch_size)):\n",
    "                valid_loss = sess.run(\n",
    "                cost, {input_data: questions_batch, targets: answers_batch, lr: learning_rate,\n",
    "                       sequence_length: answers_batch.shape[1], keep_prob: 1})\n",
    "                total_valid_loss += valid_loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "            avg_valid_loss = total_valid_loss / (len(valid_questions) / batch_size)\n",
    "            print('Valid Loss: {:>6.3f}, Seconds: {:>5.2f}'.format(avg_valid_loss, batch_time))\n",
    "\n",
    "            # Reduce learning rate, but not below its minimum value\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "\n",
    "            summary_valid_loss.append(avg_valid_loss)\n",
    "            if avg_valid_loss <= min(summary_valid_loss):\n",
    "                print('New Record!')\n",
    "                stop_early = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, checkpoint)\n",
    "\n",
    "            else:\n",
    "                print(\"No Improvement.\")\n",
    "                stop_early += 1\n",
    "                if stop_early == stop:\n",
    "                    break\n",
    "\n",
    "    if stop_early == stop:\n",
    "        print(\"Stopping Training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_seq(question, vocab_to_int):\n",
    "    \"\"\"Creating the question to be taken as input by the model\"\"\"\n",
    "    question = clean_text(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how long does a home insurance claim stay on your record\n"
     ]
    }
   ],
   "source": [
    "random = np.random.choice(len(short_questions))\n",
    "input_question = short_questions[random]\n",
    "print(input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question = question_to_seq(input_question, questions_vocab_to_int)\n",
    "input_question = input_question + [questions_vocab_to_int[\"<PAD>\"]] * (max_line_length - len(input_question))\n",
    "batch_shell = np.zeros((batch_size, max_line_length))\n",
    "batch_shell[0] = input_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_logits = sess.run(inference_logits, {input_data: batch_shell, keep_prob: 1.0})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_q = questions_vocab_to_int[\"<PAD>\"]\n",
    "pad_a = answers_vocab_to_int[\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question\n",
      "Word Ids: [3016, 4712, 5354, 5205, 2506, 5515, 44, 11179, 17338, 4619, 16337]\n",
      "Input Words: ['how', 'long', 'does', 'a', 'home', 'insurance', 'claim', 'stay', 'on', 'your', 'record']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Question')\n",
    "print('Word Ids: {}'.format([i for i in input_question if i != pad_q]))\n",
    "print('Input Words: {}'.format([questions_int_to_vocab[i] for i in input_question if i != pad_q]))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer\n",
      "Word Ids: [12817, 6907, 11308, 5205, 3610, 17027, 18628, 11308, 5205, 3610, 17027, 18628, 11034, 17664, 3264, 10176, 5515, 7648, 5205, 3610, 17027, 18628, 11034, 17664, 3264, 5205, 10176, 5515, 11263, 7648, 12817, 11263, 7742, 12190, 11399, 7742, 12817, 11263, 7742, 12190, 12817, 11263, 7742, 12190, 12817, 11263, 7742, 12190, 12817, 11263, 7742, 12190, 12817, 11263, 7742, 12190, 12817, 11263, 7742, 12190, 11399, 7742, 12817, 11263, 17329, 11034, 18948, 5205, 44, 7742, 12190, 2974, 7742, 12190, 2974, 7742, 12190, 2974, 7742, 12190, 2974, 7742, 12190, 2974, 7742, 12190, 2974, 7742, 12190, 2974, 7742, 12190, 2974, 7742, 12190, 2974, 7742, 18948, 5205, 44]\n",
      "Response Words: ['the', 'answer', 'is', 'a', 'great', 'question', 'that', 'is', 'a', 'great', 'question', 'that', 'you', 'can', 'get', 'life', 'insurance', 'for', 'a', 'great', 'question', 'that', 'you', 'can', 'get', 'a', 'life', 'insurance', 'policy', 'for', 'the', 'policy', 'to', 'be', 'paid', 'to', 'the', 'policy', 'to', 'be', 'the', 'policy', 'to', 'be', 'the', 'policy', 'to', 'be', 'the', 'policy', 'to', 'be', 'the', 'policy', 'to', 'be', 'the', 'policy', 'to', 'be', 'paid', 'to', 'the', 'policy', 'if', 'you', 'have', 'a', 'claim', 'to', 'be', 'able', 'to', 'be', 'able', 'to', 'be', 'able', 'to', 'be', 'able', 'to', 'be', 'able', 'to', 'be', 'able', 'to', 'be', 'able', 'to', 'be', 'able', 'to', 'be', 'able', 'to', 'have', 'a', 'claim']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nAnswer')\n",
    "print('Word Ids: {}'.format([i for i in np.argmax(answer_logits, 1) if i != pad_a]))\n",
    "\n",
    "print('Response Words: {}'.format([answers_int_to_vocab[i] for i in np.argmax(answer_logits, 1) if i != pad_a]))\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can life insurance be a tax deduction\n",
      "yes you can get a great question that you can get life insurance for a great question that you can get a life insurance policy for the policy to be paid to the policy to be paid to the policy to be the policy to be paid to the policy to be paid to the policy to be paid to the policy if you have a claim to be able to be able to be able to be able to be able to be able to be able to be able to be able to have a claim to be\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(([questions_int_to_vocab[i] for i in input_question if i != pad_q])))\n",
    "\n",
    "print(' '.join(([answers_int_to_vocab[i] for i in np.argmax(answer_logits, 1) if i != pad_a])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
