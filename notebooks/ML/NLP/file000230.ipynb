{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('mitre_groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", None, *patterns)\n",
    "\n",
    "doc = nlp(\"German Chancellor Angela Merkel and US President Barack\"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in [\"Angela Merkel\", \"Barack Obama\"]]\n",
    "matcher.add(\"Names\", None, *patterns)\n",
    "\n",
    "doc = nlp(\"angela merkel and us president barack Obama\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on lowercase token text:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
    "matcher.add(\"IP\", None, nlp(\"127.0.0.1\"), nlp(\"127.127.0.0\"))\n",
    "\n",
    "doc = nlp(\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on token shape:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG', 'apple'), ('San Francisco', 'GPE', 'san-francisco')]\n",
      "[('Apple', 'ORG', 'apple'), ('San Fran', 'GPE', 'san-francisco')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = English()\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [\n",
    "            {\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"}\n",
    "           ]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc1 = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\n",
    "\n",
    "doc2 = nlp(\"Apple is opening its first big office in San Fran.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('apt_group_names.txt','r+',encoding='utf-8')\n",
    "a=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"label\": \"ORG\", \"pattern\": \"Apple\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('apt_group_corpus1.jsonl','w+',encoding='utf-8')\n",
    "for i in a:\n",
    "    i=i.replace('\\n','')\n",
    "    f.write('{\"label\":\"threat_group\",\"pattern\":'+'\\\"'+i+'\\\"'+'}'+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('m1.txt','r+',encoding='utf-8')\n",
    "f2=open('rule_corpus.jsonl','a+',encoding='utf-8')\n",
    "\n",
    "a=f.readlines()\n",
    "for i in a:\n",
    "    i=i.replace('\\n','')\n",
    "    f2.write('{\"label\":\"MALWARE\",\"pattern\":[{\"LOWER\":'+'\\\"'+i.lower()+'\\\"'+'}]}'+'\\n')\n",
    "\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "new_ruler = EntityRuler(nlp).from_disk(\"apt_group_corpus.jsonl\")\n",
    "new_ruler1 = EntityRuler(nlp).from_disk(\"malware_corpus.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('China', 'GPE', ''), ('Japanese', 'NORP', ''), ('Taiwanese', 'NORP', '')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"android.deeveemap is a China-based threat group that has Trojan.jectin spearphishing campaigns targeting Japanese and Taiwanese organizations\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    text = ' '.join([ lemma.lemmatize(word,pos=wordnet.VERB) for word in text.split() if word not in stop_words ])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'technical'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation=list(punctuation)\n",
    "#punctuation.remove('.')\n",
    "\n",
    "stop_words=stopwords.words(\"english\")+ list(punctuation)\n",
    "lemmatize_text('technical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "#proide a word to be stemmed\n",
    "print(\"Porter Stemmer\")\n",
    "print(lancaster.stem(\"financial\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
